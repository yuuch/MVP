{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import separate_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Phylo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biom\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'DOG_IDB'\n",
    "# test program\n",
    "try:\n",
    "    assert flag == 'test'\n",
    "    biom_path = 'upload_files/feature-table.biom'\n",
    "    tree_path = 'upload_files/tree.nwk'\n",
    "    metadata_path = 'upload_files/demo_metadata.tsv'\n",
    "    obj_col = 'BodySite'\n",
    "    SampleID = '#SampleID'\n",
    "except:\n",
    "    # DOG_IDB data\n",
    "    biom_path = 'Dog_IDB/all.biom'\n",
    "    tree_path = 'Dog_IDB/tree.nwk'\n",
    "    metadata_path = 'Dog_IDB/833_20180418-110621.txt'\n",
    "    obj_col = 'disease_stat'\n",
    "    SampleID = 'sample_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = biom.load_table(biom_path).to_dataframe().T.to_dense()\n",
    "old_df = old_df.divide(old_df.sum(axis=1),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no #q2:types exist\n",
      "2457.5571887493134\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "mvp_tree = separate_tree.get_mvp_tree(biom_path,\\\n",
    "        tree_path,metadata_path)\n",
    "new_tic = time.time()\n",
    "# time used to gen mvp tree\n",
    "print(new_tic-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_mvp_tree = copy.deepcopy(mvp_tree.feature_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_terminals_num = len(backup_mvp_tree.get_terminals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corr_tree_new\n",
    "import pandas as pd \n",
    "import copy \n",
    "import numpy as np \n",
    "import sklearn\n",
    "def get_terminal_length(tree):\n",
    "    terminal_lengths = {}\n",
    "    for terminal in tree.get_terminals():\n",
    "        path = tree.get_path(terminal)\n",
    "        TBL = 0 # terminal branch length\n",
    "        for ele in path:\n",
    "            TBL += ele.branch_length\n",
    "        terminal_lengths[terminal] = TBL\n",
    "    sorted_terminal = sorted(terminal_lengths,key=terminal_lengths.get,reverse=True)\n",
    "    sorted_terminal_lengths ={}\n",
    "    for ele in sorted_terminal:\n",
    "        sorted_terminal_lengths[ele] = terminal_lengths[ele]\n",
    "   # print(sorted_terminal_lengths)\n",
    "    return sorted_terminal_lengths\n",
    "\n",
    "def cut_tree(tree,cp=0.2):\n",
    "    \"\"\" \n",
    "    cut some branches of the tree.\n",
    "    return the pruned branches.\n",
    "    Args:\n",
    "        tree: a Phylo.tree object\n",
    "        cp: a percent used to cut the edge.\n",
    "    \"\"\"\n",
    "    cp = 1-cp\n",
    "    clades = []\n",
    "    const = 1 # limit the terminal of a subtree\n",
    "    terminal_lengths = get_terminal_length(tree)\n",
    "    for terminal in terminal_lengths:\n",
    "        current_terminals = tree.get_terminals()\n",
    "        if not terminal in current_terminals:\n",
    "            continue\n",
    "        threshold = terminal_lengths[terminal]*cp\n",
    "        temp = 0\n",
    "        path = tree.get_path(terminal)\n",
    "        for ele in path:\n",
    "            temp += ele.branch_length\n",
    "            if temp > threshold:\n",
    "                #if not ele in current_terminals:  \n",
    "                if len(ele.get_terminals()) > const:\n",
    "                    backup = copy.copy(ele)\n",
    "                    ele.clades = []\n",
    "                    clades.append(backup)\n",
    "                    try:\n",
    "                        ele_parent = tree.get_path(ele)[-2]\n",
    "                        ele_parent.clades.remove(ele)\n",
    "                    except:\n",
    "                        pass\n",
    "                break\n",
    "    return clades\n",
    "def recurse_to_update(node):\n",
    "    \"\"\"used in the update sample series function.\n",
    "    \"\"\"\n",
    "    if node.clades:\n",
    "        node.sample_series = copy.deepcopy(recurse_to_update(node.clades[0]))\n",
    "        for i,ele in enumerate(node.clades):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            else:\n",
    "                node.sample_series += recurse_to_update(ele)\n",
    "    else:\n",
    "        return node.sample_series\n",
    "def recursion_tree(node):\n",
    "        \"\"\"recursion to get the sample_series of a tree\n",
    "        \"\"\"\n",
    "        if node.clades: # for non-leaf node\n",
    "            tmp = 0\n",
    "            flag = 0\n",
    "            for clade in node.clades:\n",
    "                if flag == 0:\n",
    "                    tmp = copy.copy(recursion_tree(clade).sample_series)\n",
    "                else:\n",
    "                    tmp += recursion_tree(clade).sample_series   \n",
    "                flag = 1\n",
    "            node.sample_series = tmp\n",
    "        else: # leaf node which has been init above.\n",
    "            try:\n",
    "                a = node.sample_series\n",
    "                #print(node.name +' is a leaf')\n",
    "            except:\n",
    "                print('please initialize the tree leaves by otu table.')\n",
    "        return node    \n",
    "def update_sample_series(tree):\n",
    "    \"\"\" update sample series after cut the tree.\n",
    "    Args:\n",
    "        tree: tree with sample_siries on the terminals.\n",
    "    \"\"\"\n",
    "    recursion_tree(tree)\n",
    "    #return tree\n",
    "    \n",
    "def get_mvp_tree(feature_table_path, tree_path, metadata_path):\n",
    "    mvp_tree = corr_tree_new.MvpTree(feature_table_path, tree_path, metadata_path)\n",
    "    return mvp_tree\n",
    "def sep_mvp_tree(mvp_tree,cp):\n",
    "    tree = copy.copy(mvp_tree.feature_tree)\n",
    "    sub_trees = cut_tree(tree,cp)\n",
    "    for ele in sub_trees:\n",
    "        ele = update_sample_series(ele)\n",
    "    print('len sub_trees:')\n",
    "    print(len(sub_trees))\n",
    "    return sub_trees\n",
    "\n",
    "def get_node_score(node,coef=[1,1,1]):\n",
    "    score = coef[0]*np.mean(node.sample_series)+\\\n",
    "        coef[1]*node.GI+coef[2]*np.std(node.sample_series)\n",
    "    return score     \n",
    "\n",
    "def search_sub_tree(sub_tree,coef,list1 = []):\n",
    "    #print('search_sub_tree')\n",
    "    p_score  = get_node_score(sub_tree,coef)\n",
    "    child_score = 0\n",
    "    for ele in sub_tree.clades:\n",
    "        child_score += get_node_score(ele,coef)\n",
    "    try:\n",
    "        child_score = child_score/len(sub_tree.clades)\n",
    "    except:\n",
    "        pass\n",
    "    if p_score <= child_score: \n",
    "        for ele in sub_tree.clades:\n",
    "            search_sub_tree(ele,coef,list1)\n",
    "    else:\n",
    "        list1.append(sub_tree)\n",
    "    #print('len list1')\n",
    "    #print(len(list1))\n",
    "    return list1\n",
    "\n",
    "    \n",
    "def generate_new_OTU_table(sub_trees,coef):\n",
    "    \"\"\"get a OTU table from the root node of the subtrees.\n",
    "    Args:\n",
    "        list1: obtained from the search_sub_tree function.\n",
    "        coef: used for compute the score function,e.g.[1,1,1]\n",
    "    \"\"\"\n",
    "    series = []\n",
    "    for sub_tree in sub_trees:\n",
    "        list1 = search_sub_tree(sub_tree,coef,list1=[])\n",
    "        for ele in list1:\n",
    "            series.append(ele.sample_series)\n",
    "    df = pd.DataFrame(series)\n",
    "    print('df.shape',df.shape)\n",
    "    #print('df.index.name',df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cp = 0.05\n",
    "coef = [1,200,1]\n",
    "sub_trees = sep_mvp_tree(mvp_tree,cp=cp)\n",
    "df = generate_new_OTU_table(sub_trees,coef).T\n",
    "mvp_tree.feature_tree =copy.deepcopy(backup_mvp_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3434"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mvp_tree.feature_tree.get_terminals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_path, sep= '\\t')\n",
    "\n",
    "metadata = metadata.set_index(SampleID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table_X = []\n",
    "feature_table_Y = []\n",
    "for ele in mvp_tree.feature_table.T.index:\n",
    "    if metadata[obj_col][ele] == 'acute hem. diarrhea':\n",
    "        continue\n",
    "    feature_table_X.append(mvp_tree.feature_table.T.loc[ele])\n",
    "    feature_table_Y.append(metadata[obj_col][ele])\n",
    "            #clf = GradientBoostingClassifier(n_estimators=100,random_state=1) # 10 estimators 0.76 ,100 Ests 0.79\n",
    "ol_clf = RandomForestClassifier(n_estimators=100, random_state=1) # 20 Ests.0.77, 100 Ests 0.79\n",
    "            #clf = AdaBoostClassifier(random_state=1) # 0.73\n",
    "            #clf = MultinomialNB(alpha=1)\n",
    "old_cv_result = cross_validate(clf,feature_table_X,feature_table_Y,cv=10,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82998366013071878"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(old_cv_result['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acute hem. diarrhea\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acute hem. diarrhea': 15, 'healthy': 98, 'IBD': 79}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = {}\n",
    "flag = True\n",
    "for ele in metadata[obj_col]:\n",
    "    if ele in categories:\n",
    "        categories[ele] += 1\n",
    "    else:\n",
    "        categories[ele] = 1\n",
    "    if ele == 'acute hem. diarrhea' and flag:\n",
    "        print(ele)\n",
    "        flag = False\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len sub_trees:\n",
      "179\n",
      "df.shape (190, 182)\n",
      "cps [0.060000000000000005, 0.07, 0.08]\n",
      "coefs [[2.0, 6.0, 13.0], [2.0, 6.0, 7.0], [2.0, 2.0, 13.0], [2.0, 2.0, 7.0], [0.0, 6.0, 13.0], [0.0, 6.0, 7.0], [0.0, 2.0, 13.0], [0.0, 2.0, 7.0]]\n",
      "len sub_trees:\n",
      "208\n",
      "df.shape (219, 182)\n",
      "df.shape (219, 182)\n",
      "df.shape (218, 182)\n",
      "df.shape (219, 182)\n",
      "df.shape (219, 182)\n",
      "df.shape (219, 182)\n",
      "df.shape (218, 182)\n",
      "df.shape (219, 182)\n",
      "len sub_trees:\n",
      "179\n",
      "df.shape (190, 182)\n",
      "df.shape (190, 182)\n",
      "df.shape (188, 182)\n",
      "df.shape (188, 182)\n",
      "df.shape (190, 182)\n",
      "df.shape (190, 182)\n",
      "df.shape (188, 182)\n",
      "df.shape (188, 182)\n",
      "len sub_trees:\n",
      "161\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "cps [0.07, 0.08, 0.09]\n",
      "coefs [[3.0, 8.0, 16.0], [3.0, 8.0, 10.0], [3.0, 4.0, 16.0], [3.0, 4.0, 10.0], [1.0, 8.0, 16.0], [1.0, 8.0, 10.0], [1.0, 4.0, 16.0], [1.0, 4.0, 10.0]]\n",
      "len sub_trees:\n",
      "179\n",
      "df.shape (190, 182)\n",
      "df.shape (190, 182)\n",
      "df.shape (188, 182)\n",
      "df.shape (190, 182)\n",
      "df.shape (190, 182)\n",
      "df.shape (190, 182)\n",
      "df.shape (188, 182)\n",
      "df.shape (190, 182)\n",
      "len sub_trees:\n",
      "161\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "df.shape (170, 182)\n",
      "len sub_trees:\n",
      "127\n",
      "df.shape (132, 182)\n",
      "df.shape (132, 182)\n",
      "df.shape (132, 182)\n",
      "df.shape (132, 182)\n",
      "df.shape (132, 182)\n",
      "df.shape (132, 182)\n",
      "df.shape (132, 182)\n",
      "df.shape (132, 182)\n",
      "[0.08, 2.0, 6.0, 13.0]\n",
      "iter_num: 3\n"
     ]
    }
   ],
   "source": [
    "#f = open('sep_mvp_result.txt','w')\n",
    "tic = time.time()\n",
    "cps = [0.07]\n",
    "#cps = [0.08]\n",
    "#[1,800,200]\n",
    "coefs = [#[1,20,1],\n",
    "         #[1,15,1],\n",
    "         #[1,10,7],\n",
    "         #[1,1,7],\n",
    "     #[1,1,10],\n",
    "    [1,4,10]\n",
    "     #[1,1,66]\n",
    "       ]\n",
    "acc_threshold = 0.5\n",
    "continue_flag = True\n",
    "cp_delta = 0.01\n",
    "deltas = [1,2,3]\n",
    "iter_num = 0\n",
    "# up down bounds used to constrain the cp(cut percent) value.\n",
    "up_bound = 0.15\n",
    "down_bound = 0.01\n",
    "# Max_key used to store the key.e.g str([0.08,1,20,1])\n",
    "Max_key = ''\n",
    "Last_Max_key = 'temp'\n",
    "results = {}\n",
    "while continue_flag:\n",
    "    iter_num += 1\n",
    "    for cp in cps:\n",
    "        mvp_tree.feature_tree =copy.deepcopy(backup_mvp_tree)\n",
    "        if len(mvp_tree.feature_tree.get_terminals()) < init_terminals_num:\n",
    "            print('tree has been edited')\n",
    "            break\n",
    "        sub_trees = sep_mvp_tree(mvp_tree,cp=cp)\n",
    "        for coef in coefs:\n",
    "            if str([cp]+[coef]) in results:\n",
    "                continue\n",
    "            df = generate_new_OTU_table(sub_trees,coef).T\n",
    "            X = []\n",
    "            Y = []\n",
    "            if df.shape == (0,0):\n",
    "                continue\n",
    "            for ele in df.index:\n",
    "                if metadata[obj_col][ele] == 'acute hem. diarrhea':\n",
    "                    continue\n",
    "                X.append(df.loc[ele])\n",
    "                Y.append(metadata[obj_col][ele])\n",
    "            #clf = GradientBoostingClassifier(n_estimators=100,random_state=1) # 10 estimators 0.76 ,100 Ests 0.79\n",
    "            clf = RandomForestClassifier(n_estimators=100, random_state=1) # 20 Ests.0.77, 100 Ests 0.79\n",
    "            #clf = AdaBoostClassifier(random_state=1) # 0.73\n",
    "            #clf = MultinomialNB(alpha=1)\n",
    "            cv_result = cross_validate(clf,X,Y,cv=10,return_train_score=True)\n",
    "            temp_key = str([cp]+coef)\n",
    "            results[temp_key]=cv_result\n",
    "    # generate new cps and coefs\n",
    "    for key in results:\n",
    "        if np.mean(results[key]['test_score']) > acc_threshold:\n",
    "            acc_threshold = np.mean(results[key]['test_score'])\n",
    "            Max_key = key\n",
    "        else:\n",
    "            pass\n",
    "    if Max_key == Last_Max_key:\n",
    "        print(Max_key)\n",
    "        break\n",
    "    Last_Max_key = copy.deepcopy(Max_key)\n",
    "    Max_key_list = [float(ele) for ele in Max_key[1:-1].split(', ')]\n",
    "    # update cps\n",
    "    cps  = [max(Max_key_list[0]-cp_delta,down_bound), Max_key_list[0], min(Max_key_list[0]+cp_delta,up_bound)]\n",
    "    # update coef\n",
    "    Max_key_list.remove(Max_key_list[0])\n",
    "    temps = [] # store paied coefs\n",
    "    for i,ele  in enumerate(Max_key_list):\n",
    "        temps.append([ele+deltas[i],ele-deltas[i]])\n",
    "    coefs = list(itertools.product(temps[0],temps[1],temps[2]))\n",
    "    coefs = [list(ele) for ele in coefs]\n",
    "    #coef.append(temp)\n",
    "    #print(results)\n",
    "    print('cps',cps)\n",
    "    print('coefs',coefs)\n",
    "print('iter_num:',iter_num)\n",
    "new_tic = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape == (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total spend 15.27 minutes\n"
     ]
    }
   ],
   "source": [
    "print('total spend %.2f minutes' %((new_tic-tic)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87512254901960773"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(results[Max_key]['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83333333,  0.88888889,  0.94444444,  0.88235294,  0.76470588,\n",
       "        0.8125    ,  1.        ,  0.9375    ,  0.875     ,  0.8125    ])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[Max_key]['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.08, 2.0, 6.0, 13.0]'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Max_key"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
